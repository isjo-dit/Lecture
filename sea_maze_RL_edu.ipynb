{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bfa4c93",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -U \"stable-baselines3[extra]\" \"gymnasium\"\n",
    "\n",
    "# ---- 항해 궤적 최적화 ----\n",
    "import os\n",
    "import gymnasium as gym              \n",
    "from gymnasium import spaces         \n",
    "import numpy as np  \n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn as nn                \n",
    "import torch                         \n",
    "import logging\n",
    "from gymnasium.wrappers import TimeLimit\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.monitor import Monitor\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv, VecTransposeImage, VecNormalize\n",
    "from stable_baselines3.common.torch_layers import BaseFeaturesExtractor\n",
    "from stable_baselines3.common.callbacks import EvalCallback, BaseCallback, CallbackList\n",
    "\n",
    "max_episode_steps = 100\n",
    "class SmallCNN(BaseFeaturesExtractor):\n",
    "    def __init__(self, observation_space, features_dim=128):\n",
    "        super().__init__(observation_space, features_dim)\n",
    "\n",
    "        C, H, W = observation_space.shape  # CHW\n",
    "\n",
    "        self.cnn = nn.Sequential(\n",
    "            nn.Conv2d(C, 32, kernel_size=3, stride=1, padding=1), nn.ReLU(),\n",
    "            nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1), nn.ReLU(),\n",
    "            nn.Flatten(),\n",
    "        )\n",
    "\n",
    "        with torch.no_grad():\n",
    "            n_flat = self.cnn(torch.zeros(1, C, H, W)).shape[1]\n",
    "\n",
    "        self.linear = nn.Sequential(\n",
    "            nn.Linear(n_flat, features_dim),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.linear(self.cnn(x))\n",
    "\n",
    "\n",
    "class SeaNavEnvCNN(gym.Env):\n",
    "    metadata = {\"render_modes\": [\"rgb_array\"], \"render_fps\": 8}\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        size=15,                 \n",
    "        obstacle_ratio=0.2,      \n",
    "        view=9,                  \n",
    "        seed=2025,               \n",
    "        map_seed=2025,           \n",
    "        fixed_start=(2, 2),      \n",
    "        fixed_goal=(12, 12),     \n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.rng = np.random.default_rng(seed)\n",
    "        self.size, self.view = size, view\n",
    "        self.obstacle_ratio = obstacle_ratio\n",
    "        self.map_seed = map_seed\n",
    "        self.fixed_start = np.array(fixed_start, np.int32)\n",
    "        self.fixed_goal  = np.array(fixed_goal,  np.int32)\n",
    "\n",
    "        self.moves = np.array(\n",
    "            [[-1, 0],  \n",
    "             [ 1, 0],  \n",
    "             [ 0,-1],  \n",
    "             [ 0, 1]], \n",
    "            np.int32\n",
    "        )\n",
    "        self.action_space = spaces.Discrete(4)\n",
    "\n",
    "        self.observation_space = spaces.Box(\n",
    "            low=0, high=255, shape=(view, view, 3), dtype=np.uint8\n",
    "        )\n",
    "        self._build_static()\n",
    "\n",
    "    def _dist(self, a, b):\n",
    "        return np.abs(a - b).sum()\n",
    "\n",
    "    def _build_static(self):\n",
    "        rng = np.random.default_rng(self.map_seed)\n",
    "        self.map = np.zeros((self.size, self.size), np.int8)\n",
    "        self.map[rng.random((self.size, self.size)) < self.obstacle_ratio] = 1\n",
    "        self.map[0, :] = self.map[-1, :] = 1\n",
    "        self.map[:, 0] = self.map[:, -1] = 1\n",
    "\n",
    "    def reset(self, *, seed=None, options=None):\n",
    "        if seed is not None:\n",
    "            self.rng = np.random.default_rng(seed)\n",
    "\n",
    "        self.map[self.fixed_start[0], self.fixed_start[1]] = 0\n",
    "        self.map[self.fixed_goal[0],  self.fixed_goal[1]]  = 0\n",
    "        self.start = self.fixed_start.copy()\n",
    "        self.goal  = self.fixed_goal.copy()\n",
    "        self.pos = self.start.copy()\n",
    "        self.prev_dist = self._dist(self.pos, self.goal)\n",
    "        self.t = 0\n",
    "\n",
    "        return self._obs(), {}\n",
    "\n",
    "    def _obs(self):\n",
    "        v = self.view\n",
    "        r = v // 2\n",
    "        y, x = self.pos\n",
    "\n",
    "        y0, y1 = y - r, y + r + 1\n",
    "        x0, x1 = x - r, x + r + 1\n",
    "\n",
    "        patch = np.ones((v, v), np.float32)\n",
    "        ys0, ys1 = max(0, y0), min(self.size, y1)\n",
    "        xs0, xs1 = max(0, x0), min(self.size, x1)\n",
    "        py0, py1 = ys0 - y0, ys0 - y0 + (ys1 - ys0)\n",
    "        px0, px1 = xs0 - x0, xs0 - x0 + (xs1 - xs0)\n",
    "        patch[py0:py1, px0:px1] = self.map[ys0:ys1, xs0:xs1]\n",
    "\n",
    "        free = 1.0 - patch\n",
    "\n",
    "        gvec = (self.goal - self.pos).astype(np.float32)\n",
    "        gdir = gvec / (np.linalg.norm(gvec) + 1e-8) if np.any(gvec) else np.array([0.0, 0.0], np.float32)\n",
    "\n",
    "        dx = np.full((v, v), gdir[1], np.float32)  # x방향 성분\n",
    "        dy = np.full((v, v), gdir[0], np.float32)  # y방향 성분\n",
    "\n",
    "        free_u8 = (free * 255.0).astype(np.uint8)\n",
    "        dx_u8   = (((dx + 1.0) * 0.5) * 255.0).astype(np.uint8)\n",
    "        dy_u8   = (((dy + 1.0) * 0.5) * 255.0).astype(np.uint8)\n",
    "\n",
    "        return np.stack([free_u8, dx_u8, dy_u8], axis=-1)\n",
    "\n",
    "    def step(self, action: int):\n",
    "        self.t += 1\n",
    "        reward = -1\n",
    "\n",
    "        move = self.moves[action]\n",
    "        nxt = self.pos + move\n",
    "\n",
    "        nxt[0] = np.clip(nxt[0], 0, self.size - 1)\n",
    "        nxt[1] = np.clip(nxt[1], 0, self.size - 1)\n",
    "\n",
    "        if self.map[nxt[0], nxt[1]] == 1:\n",
    "            reward -= 3.0\n",
    "            nxt = self.pos\n",
    "\n",
    "        new_dist = self._dist(nxt, self.goal)\n",
    "        reward += 0.2 * (self.prev_dist - new_dist)\n",
    "        self.prev_dist = new_dist\n",
    "        self.pos = nxt\n",
    "\n",
    "        done = False\n",
    "        if np.array_equal(self.pos, self.goal):\n",
    "            reward += 5.0\n",
    "            done = True\n",
    "        if self.t >= max_episode_steps:\n",
    "            done = True\n",
    "\n",
    "        return self._obs(), reward, done, False, {}\n",
    "\n",
    "    def render(self):\n",
    "        grid = np.zeros((self.size, self.size, 3), np.float32)\n",
    "        grid[:, :, 2] = 0.6                                             # 파랑\n",
    "        grid[self.map == 1] = np.array([0.8, 0.8, 0.8])                 # 회색\n",
    "        grid[self.goal[0], self.goal[1]] = np.array([1.0, 0.25, 0.25])  # 빨강\n",
    "        grid[self.pos[0],  self.pos[1]]  = np.array([1.0, 1.0, 0.2])    # 노랑\n",
    "        return (np.clip(grid, 0, 1) * 255).astype(np.uint8)\n",
    "\n",
    "class PathLogger(gym.Wrapper):\n",
    "    def __init__(self, env):\n",
    "        super().__init__(env)\n",
    "        self.path = []; self.ret = 0.0\n",
    "    def reset(self, *, seed=None, options=None):\n",
    "        obs, info = self.env.reset(seed=seed, options=options)\n",
    "        y, x = self.env.pos; self.path = [(int(y), int(x))]; self.ret = 0.0\n",
    "        return obs, info\n",
    "    def step(self, action):\n",
    "        obs, r, terminated, truncated, info = self.env.step(action)\n",
    "        y, x = self.env.pos; self.path.append((int(y), int(x))); self.ret += float(r)\n",
    "        if terminated and np.array_equal(self.env.pos, self.env.goal):\n",
    "            info = dict(info)\n",
    "            info[\"success\"] = True\n",
    "            info[\"steps\"] = len(self.path) - 1\n",
    "            info[\"return\"] = self.ret\n",
    "            info[\"path\"] = self.path.copy()\n",
    "        return obs, r, terminated, truncated, info\n",
    "\n",
    "log_dir = \"./sea_rl_logs\"; os.makedirs(log_dir, exist_ok=True)\n",
    "goal_logger = logging.getLogger(\"goal_logger\")\n",
    "goal_logger.setLevel(logging.INFO)\n",
    "goal_logger.handlers.clear(); goal_logger.propagate = False\n",
    "fmt = logging.Formatter(\"%(asctime)s %(message)s\")\n",
    "ch = logging.StreamHandler(); ch.setFormatter(fmt)\n",
    "goal_logger.addHandler(ch)\n",
    "\n",
    "class GoalEpisodeLogger(BaseCallback):\n",
    "    def __init__(self, ext_logger):\n",
    "        super().__init__(verbose=0)\n",
    "        self.ext_logger = ext_logger\n",
    "        self.ep = 0  # 전역 에피소드 번호(단일 env 가정)\n",
    "\n",
    "    def _on_step(self) -> bool:\n",
    "        infos = self.locals.get(\"infos\")\n",
    "        dones = self.locals.get(\"dones\")\n",
    "        if not infos:\n",
    "            return True\n",
    "\n",
    "        info = infos[0]\n",
    "        done = bool(dones[0]) if dones is not None else False\n",
    "\n",
    "        if info and info.get(\"success\"):\n",
    "            self.ext_logger.info(f\"[GOAL] episode={self.ep+1} steps={info['steps']} timesteps={self.num_timesteps} return={info['return']:.2f}\")\n",
    "            converted_path = list(map(list, info[\"path\"]))\n",
    "            goal_logger.info(f\"[GOAL] path for webinput: {converted_path}\")\n",
    "\n",
    "            self.logger.record(\"goal/episode\", self.ep+1)\n",
    "            self.logger.record(\"goal/steps\", info[\"steps\"])\n",
    "            self.logger.record(\"goal/return\", info[\"return\"])\n",
    "\n",
    "        if done:\n",
    "            self.ep += 1\n",
    "\n",
    "        return True\n",
    "\n",
    "class AttachGoalFileHandler(BaseCallback):\n",
    "    def __init__(self, ext_logger, base_dir, fmt):\n",
    "        super().__init__(0)\n",
    "        self.ext_logger = ext_logger\n",
    "        self.base_dir = base_dir\n",
    "        self.fmt = fmt\n",
    "        self.attached = False\n",
    "\n",
    "    def _on_training_start(self) -> None:\n",
    "        run_dir = self.model.logger.get_dir() or self.base_dir\n",
    "        os.makedirs(run_dir, exist_ok=True)\n",
    "        fh = logging.FileHandler(os.path.join(run_dir, \"goal_hits.log\"))\n",
    "        fh.setFormatter(self.fmt)\n",
    "        self.ext_logger.addHandler(fh)\n",
    "        self.attached = True\n",
    "    \n",
    "    def _on_step(self) -> bool:\n",
    "        return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5104d6be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 환경 설정\n",
    "SCENARIO = dict(size=20, obstacle_ratio=0.3, view=9, map_seed=2025, fixed_start=(1,1), fixed_goal=(17, 18))\n",
    "def make_env():\n",
    "    def _t():\n",
    "        env = SeaNavEnvCNN(**SCENARIO)\n",
    "        env = PathLogger(env)                      \n",
    "        env = TimeLimit(env, max_episode_steps=max_episode_steps)\n",
    "        return Monitor(env)\n",
    "    return _t\n",
    "\n",
    "policy_kwargs = dict(\n",
    "    features_extractor_class=SmallCNN,\n",
    "    features_extractor_kwargs=dict(features_dim=128),\n",
    "    normalize_images=True,  # uint8 입력이면 True\n",
    ")\n",
    "\n",
    "train_env = DummyVecEnv([make_env()])\n",
    "train_env = VecTransposeImage(train_env)       # HWC -> CHW\n",
    "train_env = VecNormalize(train_env, norm_obs=False, norm_reward=True)\n",
    "\n",
    "eval_env  = DummyVecEnv([make_env()])\n",
    "eval_env  = VecTransposeImage(eval_env)\n",
    "eval_env  = VecNormalize(eval_env, training=False, norm_obs=False, norm_reward=False)\n",
    "eval_env.ret_rms = train_env.ret_rms           # 보상 통계 공유\n",
    "\n",
    "mon = train_env.envs[0].env          # Monitor\n",
    "base = mon.env.unwrapped             # SeaNavEnvCNN\n",
    "base.reset(seed=2025)\n",
    "img = base.render()\n",
    "plt.figure(figsize=(4,4)); plt.imshow(img)\n",
    "sy, sx = base.start; gy, gx = base.goal\n",
    "plt.scatter([sx],[sy], marker='o', s=90, c='lime', edgecolors='black', linewidths=1.3, label='start')\n",
    "plt.scatter([gx],[gy], marker='*', s=150, c='red',  edgecolors='black', linewidths=1.3, label='goal')\n",
    "plt.legend(loc='center left', bbox_to_anchor=(1.02, 0.5), borderaxespad=0.0, frameon=True)\n",
    "plt.axis('off'); plt.title('maze'); plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4864ed59",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_cb = EvalCallback(eval_env, best_model_save_path=log_dir, eval_freq=5000,\n",
    "                       n_eval_episodes=10, deterministic=True)\n",
    "\n",
    "callbacks = CallbackList([\n",
    "    AttachGoalFileHandler(goal_logger, log_dir, fmt),\n",
    "    GoalEpisodeLogger(goal_logger),\n",
    "    eval_cb,\n",
    "])\n",
    "\n",
    "model = PPO(\n",
    "    \"CnnPolicy\", train_env,\n",
    "    device=\"cuda\",\n",
    "    n_steps=128, batch_size=128, learning_rate=0.0001,\n",
    "    gamma=0.7, gae_lambda=0.5, ent_coef=0.01, clip_range=0.2,\n",
    "    verbose=1, tensorboard_log=log_dir,\n",
    "    policy_kwargs=policy_kwargs,\n",
    ")\n",
    "\n",
    "model.learn(total_timesteps=500, callback=callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1984885",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 시각화: 시작점(초록), 목표점(빨강) 표기\n",
    "import numpy as np, matplotlib.pyplot as plt\n",
    "env = train_env.envs[0].unwrapped\n",
    "obs, _ = env.reset(seed=2025)\n",
    "path = [tuple(env.pos)]; ret=0.0\n",
    "for _ in range(max_episode_steps):\n",
    "    obs_chw = np.transpose(obs, (2,0,1))[None, ...]\n",
    "    action, _ = model.predict(obs_chw, deterministic=True)\n",
    "    obs, r, done, trunc, _ = env.step(int(action)); ret+=r\n",
    "    path.append(tuple(env.pos))\n",
    "    if done or trunc: break\n",
    "\n",
    "img = env.render()\n",
    "plt.figure(figsize=(4,4)); plt.imshow(img)\n",
    "ys,xs = zip(*path); plt.plot(xs, ys, c='w', linewidth=2)\n",
    "sy,sx = env.start; gy,gx = env.goal\n",
    "plt.scatter([sx],[sy], marker='o', s=90, c='lime', edgecolors='black', linewidths=1.3, label='start')\n",
    "plt.scatter([gx],[gy], marker='*', s=150, c='red',  edgecolors='black', linewidths=1.3, label='goal')\n",
    "plt.legend(loc='center left', bbox_to_anchor=(1.02, 0.5), borderaxespad=0.0, frameon=True)\n",
    "plt.axis('off'); plt.title(f\"steps={len(path)-1}, return={ret:.1f}\"); plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "test_ship",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
