{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m5IApEGaYW-h"
      },
      "source": [
        "# 강화학습(Reinforce Learning, RL)\n",
        "\n",
        "\n",
        "## 1. 환경(Environment)\n",
        "- 미로 자체가 **환경**이다.  \n",
        "- 환경은 상태(state), 행동(action), 보상(reward) 규칙으로 정의된다.\n",
        "  - **상태**: 에이전트의 현재 위치·방향(또는 시야 이미지).  \n",
        "  - **행동**: 위/아래/좌/우 이동, 회전, 전진 등.  \n",
        "  - **보상**: 목표 지점 도달 시 +1, 그 외 대부분 0, 잘못된 행동이나 시간 초과 시 0 또는 -1.  \n",
        "\n",
        "## 2. 에이전트(Agent)\n",
        "- 에이전트는 정책(policy)이라는 함수를 따라 행동을 선택한다.  \n",
        "- 정책은 단순한 표(탭형 Q-table)일 수도 있고, 신경망(Deep Q-Network, PPO 등)일 수도 있다.  \n",
        "\n",
        "## 3. 목표(Goal)\n",
        "- 목표는 **최단 시간에 목표 지점까지 도달**하는 것.  \n",
        "- 에피소드마다 시작 위치·목표 위치가 바뀔 수 있어, 일반화된 경로 찾기 전략을 학습해야 한다.  \n",
        "\n",
        "## 4. 학습 과정\n",
        "1. **탐험(Exploration)**  \n",
        "   - 처음에는 미로 구조를 모르기 때문에 무작위로 움직이며 경험을 쌓는다.\n",
        "2. **경험 누적**  \n",
        "   - (상태, 행동, 보상, 다음 상태)의 경험을 저장.  \n",
        "3. **정책 개선**  \n",
        "   - Q-러닝:\\\n",
        "$[Q(s,a) ← Q(s,a) + \\alpha \\big(r + \\gamma \\max_{a'} Q(s',a') - Q(s,a)\\big)]$\n",
        "로 Q값 업데이트.  \n",
        "   - deep RL(PPO, DQN 등):\\\n",
        "신경망을 통해 정책/가치 함수를 근사하고, 보상 합계(리턴)를 극대화하도록 파라미터 조정.  \n",
        "4. **성공 확률 증가**  \n",
        "   - 시간이 지날수록 무작위보다는 목표로 가는 경로를 더 자주 선택.  \n",
        "\n",
        "## 5. 중요 개념\n",
        "- **에피소드(Episode)**: 시작~목표 도달 or 최대 스텝 종료까지의 한 판.  \n",
        "- **보상 설계(Reward shaping)**: 희소 보상(목표 도착 시만 +1)이면 학습이 느리다. 중간 보상을 설계하면 탐색이 빨라진다.  \n",
        "- **탐험 vs 활용(Exploration vs Exploitation)**: 새로운 길을 시험할지, 아는 최단 경로를 쓸지 균형이 필요하다.  \n",
        "- **할인율(γ)**: 먼 미래 보상을 현재 가치로 얼마나 반영할지.  \n",
        "\n",
        "\n",
        "* * *\n",
        "\n",
        "결론: 미로 찾기 RL의 기본은 **환경(미로)과 보상 정의 → 탐험을 통한 데이터 수집 → 정책/가치 함수 업데이트 → 점점 목표에 빨리 도달**이다.  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "15SHrWEJX1JZ",
        "outputId": "a4d0aab4-3f9b-4a39-9296-6c3319666cf8"
      },
      "outputs": [],
      "source": [
        "# Fixed-maze shortest-path RL (Q-learning)\n",
        "\n",
        "import numpy as np\n",
        "from collections import deque\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "rng = np.random.default_rng(0)\n",
        "\n",
        "# ----- 1) 고정 미로 정의 (0: 빈칸, 1: 벽) -----\n",
        "maze = np.array([\n",
        " [1,1,1,1,1,1,1,1,1,1,1,1,1,1,1],\n",
        " [1,0,0,0,1,0,0,0,1,0,0,0,0,0,1],\n",
        " [1,0,1,0,1,0,1,0,1,0,1,1,1,0,1],\n",
        " [1,0,1,0,0,0,1,0,0,0,1,0,0,0,1],\n",
        " [1,0,1,1,1,0,1,1,1,0,1,0,1,0,1],\n",
        " [1,0,0,0,0,0,1,0,0,0,0,0,1,0,1],\n",
        " [1,0,1,1,1,0,1,0,1,1,1,0,1,0,1],\n",
        " [1,0,0,0,1,0,0,0,1,0,0,0,1,0,1],\n",
        " [1,1,1,0,1,1,1,0,1,1,1,0,1,0,1],\n",
        " [1,0,0,0,0,0,1,0,0,0,1,0,0,0,1],\n",
        " [1,0,1,1,1,0,1,1,1,0,1,1,1,0,1],\n",
        " [1,0,1,0,0,0,0,0,1,0,0,0,1,0,1],\n",
        " [1,0,1,0,1,1,1,0,1,1,1,0,1,0,1],\n",
        " [1,0,0,0,0,0,0,0,0,0,0,0,0,0,1],\n",
        " [1,1,1,1,1,1,1,1,1,1,1,1,1,1,1],\n",
        "], dtype=np.int8)\n",
        "\n",
        "H, W = maze.shape\n",
        "S = (1,1)    # 시작점\n",
        "G = (7,13)  # 목표점\n",
        "\n",
        "ACTIONS = [(0,-1),(0,1),(-1,0),(1,0)]  # 좌,우,상,하 (dx,dy)\n",
        "nA = len(ACTIONS)\n",
        "\n",
        "# ----- 2) 유틸: 상태 인덱스 변환 -----\n",
        "def to_idx(p): return p[0]*W + p[1]\n",
        "def to_xy(i): return (i//W, i%W)\n",
        "\n",
        "free = [(r,c) for r in range(H) for c in range(W) if maze[r,c]==0]\n",
        "nS = H*W\n",
        "\n",
        "# ----- 3) 환경 동작: 벽이면 제자리, 목표 도달 시 종료 -----\n",
        "def step(pos, a):\n",
        "    dx, dy = ACTIONS[a]\n",
        "    r, c = pos[0]+dy, pos[1]+dx\n",
        "    if maze[r,c] == 1:  # 벽이면 이동 실패 → 제자리\n",
        "        r, c = pos\n",
        "    next_pos = (r,c)\n",
        "    done = (next_pos == G)\n",
        "    reward = 0.0 if done else -1.0  # 스텝당 -1, 목표 0 → 최단 경로 유도\n",
        "    return next_pos, reward, done\n",
        "\n",
        "# ----- 4) 참값: BFS 최단 거리 -----\n",
        "def bfs_dist(start, goal):\n",
        "    Q = deque([start]); dist = {start:0}\n",
        "    while Q:\n",
        "        u = Q.popleft()\n",
        "        if u == goal: return dist[u]\n",
        "        for a in range(nA):\n",
        "            v,_r,_d = step(u,a)\n",
        "            if v not in dist:\n",
        "                dist[v] = dist[u]+(0 if v==u else 1)  # 벽이면 제자리(비용 0), 이동 시 1\n",
        "                Q.append(v)\n",
        "    return None\n",
        "\n",
        "shortest = bfs_dist(S,G)\n",
        "assert shortest is not None, \"경로 없음\"\n",
        "# print(f\"BFS shortest steps = {shortest}\")\n",
        "\n",
        "# ----- 5) Q-learning -----\n",
        "gamma = 0.1          \n",
        "alpha = 0.5\n",
        "eps_start, eps_end = 1.0, 0.05\n",
        "episodes = 100\n",
        "max_steps = 50\n",
        "\n",
        "Q = np.zeros((nS, nA), dtype=np.float32)\n",
        "\n",
        "def eps_greedy(s, eps):\n",
        "    if rng.random() < eps:\n",
        "        return rng.integers(nA)\n",
        "    return int(np.argmax(Q[s]))\n",
        "\n",
        "# ----- 6) 학습 결과 경로 추출(탐욕 정책) 및 검증 -----\n",
        "def greedy_path():\n",
        "    pos = S\n",
        "    path = [pos]\n",
        "    visited = set([pos])\n",
        "    for _ in range(2000):\n",
        "        s = to_idx(pos)\n",
        "        a = int(np.argmax(Q[s]))\n",
        "        nxt, _, done = step(pos, a)\n",
        "        if nxt == pos:  # 제자리 루프 방지 → 무작위 다른 행동 시도\n",
        "            for aa in np.argsort(-Q[s]):  # 값 큰 순서로\n",
        "                if aa == a: continue\n",
        "                nxt2,_,_ = step(pos, int(aa))\n",
        "                if nxt2 != pos:\n",
        "                    nxt = nxt2; break\n",
        "        pos = nxt\n",
        "        path.append(pos)\n",
        "        if pos == G: break\n",
        "        if pos in visited:  # 루프 방지\n",
        "            # 무작위 탈출\n",
        "            for aa in range(nA):\n",
        "                nxt2,_,_ = step(pos, aa)\n",
        "                if nxt2 != pos:\n",
        "                    pos = nxt2; path.append(pos); break\n",
        "        visited.add(pos)\n",
        "    return path\n",
        "\n",
        "paths_by_ep = {}  # {episode: path}\n",
        "for ep in range(1, episodes+1):\n",
        "    pos = S\n",
        "    eps = eps_end + (eps_start-eps_end)*np.exp(-ep/1000)  # 지수 감쇠\n",
        "    for t in range(max_steps):\n",
        "        s = to_idx(pos)\n",
        "        a = eps_greedy(s, eps)\n",
        "        nxt, r, done = step(pos, a)\n",
        "        s2 = to_idx(nxt)\n",
        "        td = r + gamma * (0.0 if done else np.max(Q[s2])) - Q[s,a]\n",
        "        Q[s,a] += alpha * td\n",
        "        pos = nxt\n",
        "        if done: break\n",
        "\n",
        "    # 1 에피소드마다 현재 greedy 정책 경로 저장\n",
        "    if ep % 1 == 0:\n",
        "        p = greedy_path()\n",
        "        paths_by_ep[ep] = p  # 목표 미도달이어도 그대로 기록\n",
        "\n",
        "\n",
        "\n",
        "path = greedy_path()\n",
        "# 유효 경로 길이(이동만 카운트: 같은 칸 중복 제외)\n",
        "move_steps = sum(path[i]!=path[i-1] for i in range(1,len(path)))\n",
        "print(f\"Greedy policy steps = {move_steps}, shortest = {shortest}\")\n",
        "\n",
        "assert path[-1]==G, \"목표 미도달\"\n",
        "assert move_steps == shortest, \"최단 경로 미달성(보상/학습 파라미터 조정 필요)\"\n",
        "\n",
        "# ----- 7) 결과 출력 -----\n",
        "print(\"Path coordinates:\")\n",
        "print(path)\n",
        "\n",
        "# === 에피소드별로 격자에 나눠 그리기(변화 과정 비교) ===\n",
        "def plot_paths_grid(maze, S, G, paths_by_ep, episodes=None, ncols=5):\n",
        "    import numpy as np\n",
        "    import matplotlib.pyplot as plt\n",
        "    if episodes is None:\n",
        "        episodes = sorted(paths_by_ep.keys())\n",
        "    n = len(episodes)\n",
        "    nrows = (n + ncols - 1) // ncols\n",
        "\n",
        "    H, W = maze.shape\n",
        "    fig, axes = plt.subplots(nrows, ncols, figsize=(3*ncols, 3*nrows))\n",
        "    axes = np.atleast_1d(axes).ravel()\n",
        "\n",
        "    for ax, ep in zip(axes, episodes):\n",
        "        path = paths_by_ep.get(ep, [])\n",
        "        ax.imshow(maze, cmap=\"binary\", interpolation=\"none\", origin=\"upper\")\n",
        "        ax.set_xticks([]); ax.set_yticks([])\n",
        "        ax.scatter([S[1]],[S[0]], s=60, marker=\"o\", facecolors=\"none\", edgecolors=\"tab:green\", linewidths=1.5)\n",
        "        ax.scatter([G[1]],[G[0]], s=70, marker=\"*\", color=\"tab:red\")\n",
        "        if path:\n",
        "            ys = [r for r, c in path]\n",
        "            xs = [c for r, c in path]\n",
        "            ax.plot(xs, ys, linewidth=1.5)\n",
        "        ax.set_title(f\"ep{ep}\", fontsize=9)\n",
        "\n",
        "    # 빈 축 숨기기\n",
        "    for ax in axes[n:]:\n",
        "        ax.axis(\"off\")\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "def plot_maze_path(maze, path, S, G, annotate_steps=False, title=\"Maze & Path\"):\n",
        "    \"\"\"maze: 0=빈칸, 1=벽\n",
        "       path: [(r,c), ...]  (S→G)\n",
        "       S,G : (r,c)\n",
        "    \"\"\"\n",
        "    H, W = maze.shape\n",
        "    fig, ax = plt.subplots(figsize=(max(5, W*0.6), max(5, H*0.6)))\n",
        "\n",
        "    # 벽=검정, 빈칸=흰색\n",
        "    ax.imshow(maze, cmap=\"binary\", interpolation=\"none\", origin=\"upper\")\n",
        "\n",
        "    # 격자선\n",
        "    ax.set_xticks(np.arange(-0.5, W, 1))\n",
        "    ax.set_yticks(np.arange(-0.5, H, 1))\n",
        "    ax.grid(color=\"lightgray\", linewidth=0.5)\n",
        "    ax.set_xticklabels([])\n",
        "    ax.set_yticklabels([])\n",
        "\n",
        "    # 경로 라인 (셀 중심을 통과)\n",
        "    ys = [r for r, c in path]\n",
        "    xs = [c for r, c in path]\n",
        "    ax.plot(xs, ys, linewidth=2, zorder=3, label=\"Path\")\n",
        "\n",
        "    # 시작(S), 목표(G)\n",
        "    ax.scatter([S[1]],[S[0]], s=180, marker=\"o\", facecolors=\"none\",\n",
        "               edgecolors=\"tab:green\", linewidths=2, zorder=4, label=\"Start\")\n",
        "    ax.scatter([G[1]],[G[0]], s=220, marker=\"*\", color=\"tab:red\",\n",
        "               linewidths=1.5, zorder=4, label=\"Goal\")\n",
        "\n",
        "    # 스텝 번호 주석(옵션)\n",
        "    if annotate_steps:\n",
        "        for i, (r, c) in enumerate(path):\n",
        "            if i % 2 == 0:  # 빽빽함 방지: 2스텝마다 표시\n",
        "                ax.text(c, r, str(i), ha=\"center\", va=\"center\",\n",
        "                        fontsize=8, color=\"tab:blue\", zorder=5)\n",
        "\n",
        "    ax.set_title(title)\n",
        "    ax.legend(loc=\"upper right\")\n",
        "    plt.show()\n",
        "\n",
        "plot_paths_grid(maze, S, G, paths_by_ep, episodes=[10,50,100,200,300], ncols=3)\n",
        "plot_maze_path(maze, path, S, G, annotate_steps=True)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "test_ship",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
