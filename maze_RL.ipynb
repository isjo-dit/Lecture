{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "07c43c39",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-23 14:11:02.220652: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-09-23 14:11:02.440150: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-09-23 14:11:03.119069: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== GPU 사용 가능 여부 확인 ===\n",
      "CUDA 사용 가능: True\n",
      "CUDA 버전: 12.8\n",
      "사용 가능한 GPU 개수: 1\n"
     ]
    }
   ],
   "source": [
    "# !pip install \"stable-baselines3[extra]\" gymnasium minigrid torch torchvision imageio[ffmpeg]\n",
    "# !pip install \"gymnasium[other]\"\n",
    "\n",
    "import os, shutil, glob, subprocess, random, re\n",
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import minigrid  # noqa\n",
    "from minigrid.minigrid_env import MiniGridEnv\n",
    "from minigrid.core.grid import Grid\n",
    "from minigrid.core.world_object import Wall, Goal\n",
    "from minigrid.wrappers import RGBImgObsWrapper, ImgObsWrapper, FullyObsWrapper\n",
    "from gymnasium.spaces import Text\n",
    "from gymnasium import ActionWrapper, ObservationWrapper, RewardWrapper, spaces\n",
    "from gymnasium.wrappers import RecordVideo\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.callbacks import BaseCallback\n",
    "from minigrid.core.actions import Actions\n",
    "from minigrid.core.world_object import Goal\n",
    "from collections import deque\n",
    "import torch \n",
    "\n",
    "ENV_ID    = \"MiniGrid-FourRooms-v0\"   # 19x19 기본\n",
    "VIDEO_DIR = \"videos\"\n",
    "OUT_FILE  = \"learning_x2.mp4\"\n",
    "\n",
    "print('=== GPU 사용 가능 여부 확인 ==='); \n",
    "print(f'CUDA 사용 가능: {torch.cuda.is_available()}'); \n",
    "print(f'CUDA 버전: {torch.version.cuda if torch.cuda.is_available() else \"N/A\"}')\n",
    "print(f'사용 가능한 GPU 개수: {torch.cuda.device_count()}')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "85976398",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "Wrapping the env in a VecTransposeImage.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 200      |\n",
      "|    ep_rew_mean     | -2       |\n",
      "| time/              |          |\n",
      "|    fps             | 282      |\n",
      "|    iterations      | 1        |\n",
      "|    time_elapsed    | 7        |\n",
      "|    total_timesteps | 2048     |\n",
      "---------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ffmpeg version 5.0.1 Copyright (c) 2000-2022 the FFmpeg developers\n",
      "  built with gcc 10.3.0 (conda-forge gcc 10.3.0-16)\n",
      "  configuration: --prefix=/home/isjo/miniconda3/envs/test_ship --cc=/home/conda/feedstock_root/build_artifacts/ffmpeg_1653042464189/_build_env/bin/x86_64-conda-linux-gnu-cc --disable-doc --disable-openssl --enable-demuxer=dash --enable-gnutls --enable-gpl --enable-hardcoded-tables --enable-libfreetype --enable-libopenh264 --enable-vaapi --enable-libx264 --enable-libx265 --enable-libaom --enable-libsvtav1 --enable-libxml2 --enable-libvpx --enable-pic --enable-pthreads --enable-shared --disable-static --enable-version3 --enable-zlib --enable-libmp3lame --pkg-config=/home/conda/feedstock_root/build_artifacts/ffmpeg_1653042464189/_build_env/bin/pkg-config\n",
      "  libavutil      57. 17.100 / 57. 17.100\n",
      "  libavcodec     59. 18.100 / 59. 18.100\n",
      "  libavformat    59. 16.100 / 59. 16.100\n",
      "  libavdevice    59.  4.100 / 59.  4.100\n",
      "  libavfilter     8. 24.100 /  8. 24.100\n",
      "  libswscale      6.  4.100 /  6.  4.100\n",
      "  libswresample   4.  3.100 /  4.  3.100\n",
      "  libpostproc    56.  3.100 / 56.  3.100\n",
      "Input #0, mov,mp4,m4a,3gp,3g2,mj2, from 'videos/ep_20_ts_4000/eval_ep_20_ts_4000-episode-0.mp4':\n",
      "  Metadata:\n",
      "    major_brand     : isom\n",
      "    minor_version   : 512\n",
      "    compatible_brands: isomiso2avc1mp41\n",
      "    encoder         : Lavf61.1.100\n",
      "  Duration: 00:00:20.10, start: 0.000000, bitrate: 17 kb/s\n",
      "  Stream #0:0[0x1](und): Video: h264 (High) (avc1 / 0x31637661), yuv420p(progressive), 288x288, 16 kb/s, 10 fps, 10 tbr, 10240 tbn (default)\n",
      "    Metadata:\n",
      "      handler_name    : VideoHandler\n",
      "      vendor_id       : [0][0][0][0]\n",
      "      encoder         : Lavc61.3.100 libx264\n",
      "Stream mapping:\n",
      "  Stream #0:0 -> #0:0 (h264 (native) -> h264 (libx264))\n",
      "Press [q] to stop, [?] for help\n",
      "[Parsed_drawtext_0 @ 0x55aaff2c3200] No font filename provided\n",
      "[AVFilterGraph @ 0x55aaff2c5a40] Error initializing filter 'drawtext' with args 'text=EP=20  STEPS=200 TS=4000 R=-1.72, FAIL:x=20:y=h-50:fontsize=10:fontcolor=white:box=1:boxcolor=black@0.55:boxborderw=10'\n",
      "Error reinitializing filters!\n",
      "Failed to inject frame into filter network: Invalid argument\n",
      "Error while processing the decoded data for stream #0:0\n",
      "Conversion failed!\n"
     ]
    },
    {
     "ename": "CalledProcessError",
     "evalue": "Command '['ffmpeg', '-y', '-i', 'videos/ep_20_ts_4000/eval_ep_20_ts_4000-episode-0.mp4', '-vf', \"drawtext=text='EP=20  STEPS=200 TS=4000 R=-1.72, FAIL':x=20:y=h-50:fontsize=10:fontcolor=white:box=1:boxcolor=black@0.55:boxborderw=10\", '-an', 'videos/ep_20_ts_4000/eval_ep_20_ts_4000-episode-0_labeled.mp4']' returned non-zero exit status 1.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mCalledProcessError\u001b[39m                        Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 258\u001b[39m\n\u001b[32m    256\u001b[39m model = PPO(\u001b[33m\"\u001b[39m\u001b[33mCnnPolicy\u001b[39m\u001b[33m\"\u001b[39m, train_env, verbose=\u001b[32m1\u001b[39m)\n\u001b[32m    257\u001b[39m cb = RecordEveryNEpisodes(every_episodes=\u001b[32m20\u001b[39m, video_root=VIDEO_DIR, start=START, goal=GOAL)\n\u001b[32m--> \u001b[39m\u001b[32m258\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlearn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtotal_timesteps\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m30000\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallback\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    260\u001b[39m \u001b[38;5;66;03m# 라벨본만 모아 타임라인 합치기\u001b[39;00m\n\u001b[32m    261\u001b[39m labeled = []\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/test_ship/lib/python3.11/site-packages/stable_baselines3/ppo/ppo.py:311\u001b[39m, in \u001b[36mPPO.learn\u001b[39m\u001b[34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, progress_bar)\u001b[39m\n\u001b[32m    302\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mlearn\u001b[39m(\n\u001b[32m    303\u001b[39m     \u001b[38;5;28mself\u001b[39m: SelfPPO,\n\u001b[32m    304\u001b[39m     total_timesteps: \u001b[38;5;28mint\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    309\u001b[39m     progress_bar: \u001b[38;5;28mbool\u001b[39m = \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[32m    310\u001b[39m ) -> SelfPPO:\n\u001b[32m--> \u001b[39m\u001b[32m311\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlearn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    312\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtotal_timesteps\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtotal_timesteps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    313\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcallback\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcallback\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    314\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlog_interval\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlog_interval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    315\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtb_log_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtb_log_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    316\u001b[39m \u001b[43m        \u001b[49m\u001b[43mreset_num_timesteps\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreset_num_timesteps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    317\u001b[39m \u001b[43m        \u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    318\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/test_ship/lib/python3.11/site-packages/stable_baselines3/common/on_policy_algorithm.py:324\u001b[39m, in \u001b[36mOnPolicyAlgorithm.learn\u001b[39m\u001b[34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, progress_bar)\u001b[39m\n\u001b[32m    321\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m.env \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    323\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mself\u001b[39m.num_timesteps < total_timesteps:\n\u001b[32m--> \u001b[39m\u001b[32m324\u001b[39m     continue_training = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcollect_rollouts\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallback\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mrollout_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_rollout_steps\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mn_steps\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    326\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m continue_training:\n\u001b[32m    327\u001b[39m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/test_ship/lib/python3.11/site-packages/stable_baselines3/common/on_policy_algorithm.py:224\u001b[39m, in \u001b[36mOnPolicyAlgorithm.collect_rollouts\u001b[39m\u001b[34m(self, env, callback, rollout_buffer, n_rollout_steps)\u001b[39m\n\u001b[32m    222\u001b[39m \u001b[38;5;66;03m# Give access to local variables\u001b[39;00m\n\u001b[32m    223\u001b[39m callback.update_locals(\u001b[38;5;28mlocals\u001b[39m())\n\u001b[32m--> \u001b[39m\u001b[32m224\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[43mcallback\u001b[49m\u001b[43m.\u001b[49m\u001b[43mon_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[32m    225\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m    227\u001b[39m \u001b[38;5;28mself\u001b[39m._update_info_buffer(infos, dones)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/test_ship/lib/python3.11/site-packages/stable_baselines3/common/callbacks.py:114\u001b[39m, in \u001b[36mBaseCallback.on_step\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    111\u001b[39m \u001b[38;5;28mself\u001b[39m.n_calls += \u001b[32m1\u001b[39m\n\u001b[32m    112\u001b[39m \u001b[38;5;28mself\u001b[39m.num_timesteps = \u001b[38;5;28mself\u001b[39m.model.num_timesteps\n\u001b[32m--> \u001b[39m\u001b[32m114\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_on_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 212\u001b[39m, in \u001b[36mRecordEveryNEpisodes._on_step\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    210\u001b[39m     text = \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mEP=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m.ep\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m  STEPS=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mep_steps\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m TS=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m.num_timesteps\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m R=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mep_reward\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m, \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mstatus\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    211\u001b[39m     labeled = mp4s[\u001b[32m0\u001b[39m].replace(\u001b[33m\"\u001b[39m\u001b[33m.mp4\u001b[39m\u001b[33m\"\u001b[39m,\u001b[33m\"\u001b[39m\u001b[33m_labeled.mp4\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m212\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_burn_text\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmp4s\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabeled\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    213\u001b[39m     os.remove(mp4s[\u001b[32m0\u001b[39m])\n\u001b[32m    215\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m[RECORDED] \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtag\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m R=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mep_reward\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m steps=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mep_steps\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m → \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mout_dir\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 162\u001b[39m, in \u001b[36mRecordEveryNEpisodes._burn_text\u001b[39m\u001b[34m(self, in_mp4, text, out_mp4)\u001b[39m\n\u001b[32m    159\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_burn_text\u001b[39m(\u001b[38;5;28mself\u001b[39m, in_mp4, text, out_mp4):\n\u001b[32m    160\u001b[39m     draw = (\u001b[33m\"\u001b[39m\u001b[33mdrawtext=text=\u001b[39m\u001b[33m'\u001b[39m\u001b[33m\"\u001b[39m + text.replace(\u001b[33m\"\u001b[39m\u001b[33m:\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[33m:\u001b[39m\u001b[33m\"\u001b[39m) +\n\u001b[32m    161\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33m'\u001b[39m\u001b[33m:x=20:y=h-50:fontsize=10:fontcolor=white:box=1:boxcolor=black@0.55:boxborderw=10\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m162\u001b[39m     \u001b[43msubprocess\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mffmpeg\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m-y\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m-i\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43min_mp4\u001b[49m\u001b[43m,\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m-vf\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43mdraw\u001b[49m\u001b[43m,\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m-an\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43mout_mp4\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcheck\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/test_ship/lib/python3.11/subprocess.py:571\u001b[39m, in \u001b[36mrun\u001b[39m\u001b[34m(input, capture_output, timeout, check, *popenargs, **kwargs)\u001b[39m\n\u001b[32m    569\u001b[39m     retcode = process.poll()\n\u001b[32m    570\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m check \u001b[38;5;129;01mand\u001b[39;00m retcode:\n\u001b[32m--> \u001b[39m\u001b[32m571\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m CalledProcessError(retcode, process.args,\n\u001b[32m    572\u001b[39m                                  output=stdout, stderr=stderr)\n\u001b[32m    573\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m CompletedProcess(process.args, retcode, stdout, stderr)\n",
      "\u001b[31mCalledProcessError\u001b[39m: Command '['ffmpeg', '-y', '-i', 'videos/ep_20_ts_4000/eval_ep_20_ts_4000-episode-0.mp4', '-vf', \"drawtext=text='EP=20  STEPS=200 TS=4000 R=-1.72, FAIL':x=20:y=h-50:fontsize=10:fontcolor=white:box=1:boxcolor=black@0.55:boxborderw=10\", '-an', 'videos/ep_20_ts_4000/eval_ep_20_ts_4000-episode-0_labeled.mp4']' returned non-zero exit status 1."
     ]
    }
   ],
   "source": [
    "class FourRoomsSmall(MiniGridEnv):\n",
    "    \"\"\"\n",
    "    11x11 기본. 내부 십자벽 + 네 곳 통로. 시작/목표 고정 가능.\n",
    "    \"\"\"\n",
    "    def __init__(self, size=11, agent_start_pos=(1,1), goal_pos=(9,9),\n",
    "                 start_dir=0, max_steps=200, **kwargs):\n",
    "        assert size % 2 == 1 and size >= 9, \"홀수, 최소 9 권장\"\n",
    "        self.size = size\n",
    "        self._agent_start_pos = agent_start_pos\n",
    "        self._goal_pos = goal_pos\n",
    "        self._start_dir = start_dir\n",
    "        mission_space = Text(max_length=50)\n",
    "        super().__init__(mission_space=mission_space,\n",
    "            grid_size=size, max_steps=max_steps,\n",
    "            see_through_walls=False, **kwargs)\n",
    "\n",
    "    def _gen_grid(self, width, height):\n",
    "        # 빈 그리드 생성 + 외벽\n",
    "        self.grid = Grid(width, height)\n",
    "        self.grid.wall_rect(0, 0, width, height)\n",
    "\n",
    "        # 십자 내부벽\n",
    "        midx, midy = width // 2, height // 2\n",
    "        for y in range(1, height-1):\n",
    "            if y != midy:\n",
    "                self.grid.set(midx, y, Wall())\n",
    "        for x in range(1, width-1):\n",
    "            if x != midx:\n",
    "                self.grid.set(x, midy, Wall())\n",
    "\n",
    "        # 네 통로(문) 위치: 필요시 수정\n",
    "        openings = [\n",
    "            (midx, 2),                  # 위쪽 통로\n",
    "            (midx, height - 3),         # 아래쪽 통로\n",
    "            (2, midy),                  # 왼쪽 통로\n",
    "            (width - 3, midy),          # 오른쪽 통로\n",
    "        ]\n",
    "        for x, y in openings:\n",
    "            self.grid.set(x, y, None)\n",
    "\n",
    "        # 목표 배치\n",
    "        if self._goal_pos is not None:\n",
    "            gx, gy = self._goal_pos\n",
    "            self.put_obj(Goal(), gx, gy)\n",
    "        else:\n",
    "            self.place_obj(Goal(), top=(midx+1, 1), size=(width-midx-2, height-2))\n",
    "\n",
    "        # 에이전트 시작 배치\n",
    "        if self._agent_start_pos is not None:\n",
    "            sx, sy = self._agent_start_pos\n",
    "            self.agent_pos = np.array([sx, sy], dtype=np.int64)\n",
    "            self.agent_dir = self._start_dir\n",
    "        else:\n",
    "            self.place_agent(top=(1, 1), size=(midx-2, height-2))\n",
    "\n",
    "        self.mission = \"reach the goal\"\n",
    "\n",
    "# ---------- 0) 고정 시작/목표 + 고정 레이아웃 ----------\n",
    "class FixedStartGoal(ObservationWrapper):\n",
    "    \"\"\"항상 같은 레이아웃/시작/목표로 초기화.\"\"\"\n",
    "    def __init__(self, env, start=(1,1), goal=(17,17), layout_seed=123, start_dir=0):\n",
    "        super().__init__(env)\n",
    "        self.start = tuple(start)\n",
    "        self.goal = tuple(goal)\n",
    "        self.layout_seed = int(layout_seed)\n",
    "        self.start_dir = int(start_dir)\n",
    "\n",
    "    # 필수: 관측을 그대로 통과\n",
    "    def observation(self, obs):\n",
    "        return obs\n",
    "\n",
    "    def _is_free(self, pos):\n",
    "        x, y = pos\n",
    "        g = self.unwrapped.grid.get(x, y)\n",
    "        return (g is None) or (getattr(g, \"type\", None) == \"goal\")\n",
    "\n",
    "    def _nearest_free(self, target):\n",
    "        W, H = self.unwrapped.grid.width, self.unwrapped.grid.height\n",
    "        q = deque([target]); seen = {target}\n",
    "        while q:\n",
    "            x, y = q.popleft()\n",
    "            if 0 <= x < W and 0 <= y < H and self._is_free((x,y)):\n",
    "                return (x, y)\n",
    "            for dx,dy in [(1,0),(-1,0),(0,1),(0,-1)]:\n",
    "                nx, ny = x+dx, y+dy\n",
    "                if 0 <= nx < W and 0 <= ny < H and (nx,ny) not in seen:\n",
    "                    seen.add((nx,ny)); q.append((nx,ny))\n",
    "        return target\n",
    "\n",
    "    def reset(self, **kwargs):\n",
    "        # 레이아웃 고정 seed로 초기화\n",
    "        obs, info = self.env.reset(seed=self.layout_seed)\n",
    "\n",
    "        # 목표 강제 배치\n",
    "        gx, gy = self._nearest_free(self.goal)\n",
    "        grid = self.unwrapped.grid\n",
    "        W, H = grid.width, grid.height\n",
    "        for x in range(W):\n",
    "            for y in range(H):\n",
    "                obj = grid.get(x,y)\n",
    "                if obj is not None and getattr(obj, \"type\", None) == \"goal\":\n",
    "                    grid.set(x,y,None)\n",
    "        grid.set(gx, gy, Goal())\n",
    "\n",
    "        # 시작 위치/방향 고정\n",
    "        sx, sy = self._nearest_free(self.start)\n",
    "        self.unwrapped.agent_pos = np.array([sx, sy], dtype=np.int64)\n",
    "        self.unwrapped.agent_dir = self.start_dir\n",
    "\n",
    "        # 관측 재계산\n",
    "        obs = self.unwrapped.gen_obs()\n",
    "        return obs, info\n",
    "\n",
    "class StepPenalty(RewardWrapper):\n",
    "    \"\"\"스텝당 -p, 목표 도달 시 +1.0. 기본값 p=0.01.\"\"\"\n",
    "    def __init__(self, env, step_penalty=0.01):\n",
    "        super().__init__(env)\n",
    "        self.step_penalty = float(step_penalty)\n",
    "    def reward(self, reward):\n",
    "        # MiniGrid의 성공 보상(1.0)은 유지, 그 외엔 -step_penalty\n",
    "        return reward if reward > 0 else -self.step_penalty\n",
    "\n",
    "# ---------- 1) 내비게이션 전용 액션 ----------\n",
    "class OnlyNavActions(ActionWrapper):\n",
    "    def __init__(self, env):\n",
    "        super().__init__(env)\n",
    "        self.action_space = spaces.Discrete(3)  # 0:left, 1:right, 2:forward\n",
    "    def action(self, act):\n",
    "        i = int(act) % 3\n",
    "        return {0: Actions.left, 1: Actions.right, 2: Actions.forward}[i]\n",
    "\n",
    "# ---------- 2) 환경 생성 ----------\n",
    "def make_env(render_mode=None, fully_obs=False, start=(1,1), goal=(17,17)):\n",
    "    # env = gym.make(ENV_ID, render_mode=render_mode, max_steps=100)\n",
    "    env = FourRoomsSmall(\n",
    "        size=9,                  # 9/11/13 등으로 변경 가능\n",
    "        agent_start_pos=(1,1),    # 시작 고정\n",
    "        goal_pos=(7,7),           # 목표 고정\n",
    "        start_dir=0,\n",
    "        max_steps=200,\n",
    "        render_mode=render_mode\n",
    "    )\n",
    "    env = FixedStartGoal(env, start=start, goal=goal, layout_seed=123, start_dir=0)\n",
    "    if fully_obs:\n",
    "        env = FullyObsWrapper(env)\n",
    "    env = OnlyNavActions(env)\n",
    "    env = RGBImgObsWrapper(env)\n",
    "    env = ImgObsWrapper(env)\n",
    "    env = StepPenalty(env, step_penalty=0.01)   # 최단 스텝 유도\n",
    "    return env\n",
    "\n",
    "# ---------- 3) 주기적 녹화 콜백(자막 1회만) ----------\n",
    "class RecordEveryNEpisodes(BaseCallback):\n",
    "    def __init__(self, every_episodes=50, video_root=\"videos\", start=(1,1), goal=(17,17)):\n",
    "        super().__init__(); self.every=every_episodes; self.ep=0; self.last=0\n",
    "        self.video_root = video_root; os.makedirs(video_root, exist_ok=True)\n",
    "        self.start, self.goal = start, goal\n",
    "\n",
    "    def _burn_text(self, in_mp4, text, out_mp4):\n",
    "        draw = (\"drawtext=text='\" + text.replace(\":\", \"\\\\:\") +\n",
    "                \"':x=20:y=h-50:fontsize=10:fontcolor=white:box=1:boxcolor=black@0.55:boxborderw=10\")\n",
    "        subprocess.run([\"ffmpeg\",\"-y\",\"-i\",in_mp4,\"-vf\",draw,\"-an\",out_mp4], check=True)\n",
    "        \n",
    "    def _on_step(self):\n",
    "        dones = self.locals.get(\"dones\")\n",
    "        if dones is None: return True\n",
    "        n_done = int(dones.sum()) if hasattr(dones, \"sum\") else int(bool(dones))\n",
    "        if n_done == 0: return True\n",
    "\n",
    "        self.ep += n_done\n",
    "        if self.ep - self.last < self.every: return True\n",
    "\n",
    "        tag = f\"ep_{self.ep}_ts_{self.num_timesteps}\"\n",
    "        out_dir = os.path.join(self.video_root, tag)\n",
    "        if os.path.exists(out_dir): shutil.rmtree(out_dir)\n",
    "\n",
    "        eval_env = RecordVideo(\n",
    "            make_env(render_mode=\"rgb_array\", start=self.start, goal=self.goal),\n",
    "            video_folder=out_dir,\n",
    "            episode_trigger=lambda e: e == 0,\n",
    "            name_prefix=f\"eval_{tag}\"\n",
    "        )\n",
    "        obs, _ = eval_env.reset()  # 레이아웃/시작/목표는 FixedStartGoal이 고정\n",
    "\n",
    "        # 평가(확률 정책 + 정지 방지)\n",
    "        ep_reward, done, ep_steps = 0.0, False, 0\n",
    "        last_pos = getattr(eval_env.unwrapped, \"agent_pos\", None)\n",
    "        still, MAX_STILL = 0, 4\n",
    "        while not done:\n",
    "            action, _ = self.model.predict(obs, deterministic=False)\n",
    "            obs, r, t, tr, _ = eval_env.step(action)\n",
    "            ep_reward += r; ep_steps += 1\n",
    "            cur_pos = getattr(eval_env.unwrapped, \"agent_pos\", None)\n",
    "            if np.array_equal(cur_pos, last_pos):\n",
    "                still += 1\n",
    "                if still >= MAX_STILL:\n",
    "                    obs, _, t2, tr2, _ = eval_env.step(Actions.right)\n",
    "                    ep_steps += 1\n",
    "                    t |= t2; tr |= tr2; still = 0\n",
    "            else:\n",
    "                still = 0\n",
    "            last_pos = cur_pos\n",
    "            done = t or tr\n",
    "        eval_env.close()\n",
    "\n",
    "        mp4s = sorted([p for p in glob.glob(os.path.join(out_dir,\"*.mp4\"))\n",
    "                       if not p.endswith(\"_labeled.mp4\")])\n",
    "        if mp4s:\n",
    "            status = \"SUCCESS\" if ep_reward > 0 else \"FAIL\"\n",
    "            text = f\"EP={self.ep}  STEPS={ep_steps} TS={self.num_timesteps} R={ep_reward:.2f}, {status}\"\n",
    "            labeled = mp4s[0].replace(\".mp4\",\"_labeled.mp4\")\n",
    "            self._burn_text(mp4s[0], text, labeled)\n",
    "            os.remove(mp4s[0])\n",
    "\n",
    "        print(f\"[RECORDED] {tag} R={ep_reward:.2f} steps={ep_steps} → {out_dir}\")\n",
    "        self.last = self.ep\n",
    "        return True\n",
    "\n",
    "# ---------- 4) 최단 경로(BFS) 계산해 검증 ----------\n",
    "def bfs_shortest_unwrapped(env_unwrapped):\n",
    "    grid = env_unwrapped.grid\n",
    "    W, H = grid.width, grid.height\n",
    "    sx, sy = tuple(env_unwrapped.agent_pos)\n",
    "    # goal 찾기\n",
    "    gx = gy = None\n",
    "    for x in range(W):\n",
    "        for y in range(H):\n",
    "            obj = grid.get(x, y)\n",
    "            if obj is not None and getattr(obj, \"type\", None) == \"goal\":\n",
    "                gx, gy = x, y\n",
    "    assert gx is not None\n",
    "    from collections import deque\n",
    "    q = deque([(sx, sy)]); dist = {(sx,sy):0}\n",
    "    def passable(x,y):\n",
    "        o = grid.get(x,y)\n",
    "        return (o is None) or (getattr(o,\"type\",None)==\"goal\")\n",
    "    while q:\n",
    "        x,y = q.popleft()\n",
    "        if (x,y)==(gx,gy): return dist[(x,y)]\n",
    "        for dx,dy in [(1,0),(-1,0),(0,1),(0,-1)]:\n",
    "            nx,ny = x+dx, y+dy\n",
    "            if 0<=nx<W and 0<=ny<H and passable(nx,ny) and (nx,ny) not in dist:\n",
    "                dist[(nx,ny)] = dist[(x,y)]+1; q.append((nx,ny))\n",
    "    return None\n",
    "\n",
    "# ================== 실행 ==================\n",
    "if __name__ == \"__main__\":\n",
    "    os.makedirs(VIDEO_DIR, exist_ok=True)\n",
    "\n",
    "    # 고정 좌표 지정(벽이면 래퍼가 인접 빈칸으로 자동 보정)\n",
    "    START = (1,1)\n",
    "    GOAL  = (7,7)\n",
    "\n",
    "    # 훈련\n",
    "    train_env = make_env(start=START, goal=GOAL)\n",
    "    model = PPO(\"CnnPolicy\", train_env, verbose=1)\n",
    "    cb = RecordEveryNEpisodes(every_episodes=20, video_root=VIDEO_DIR, start=START, goal=GOAL)\n",
    "    model.learn(total_timesteps=30000, callback=cb)\n",
    "\n",
    "    # 라벨본만 모아 타임라인 합치기\n",
    "    labeled = []\n",
    "    for root, _, fs in os.walk(VIDEO_DIR):\n",
    "        for f in fs:\n",
    "            if f.startswith(\"eval_\") and f.endswith(\"_labeled.mp4\"):\n",
    "                m = re.search(r\"ep_(\\d+)\", f); ep = int(m.group(1)) if m else 0\n",
    "                labeled.append((ep, os.path.join(root, f)))\n",
    "    labeled.sort(key=lambda x: x[0])\n",
    "    if labeled:\n",
    "        list_path = \"list.txt\"\n",
    "        with open(list_path,\"w\") as f:\n",
    "            for _,p in labeled: f.write(f\"file '{os.path.abspath(p)}'\\n\")\n",
    "        subprocess.run([\n",
    "        \"ffmpeg\",\"-y\",\n",
    "        \"-f\",\"concat\",\"-safe\",\"0\",\n",
    "        \"-i\",list_path,\n",
    "        \"-filter:v\",\"setpts=0.5*PTS\",\n",
    "        \"-an\",\n",
    "        OUT_FILE], check=True)\n",
    "        print(f\"학습 타임라인: {OUT_FILE}\")\n",
    "    else:\n",
    "        print(\"라벨본 없음\")\n",
    "\n",
    "    # 최단 경로 길이와 정책 성능 비교\n",
    "    check_env = make_env(render_mode=None, start=START, goal=GOAL)\n",
    "    check_env.reset()\n",
    "    shortest = bfs_shortest_unwrapped(check_env.unwrapped)\n",
    "    print(f\"BFS shortest steps = {shortest}\")\n",
    "\n",
    "    # 학습 정책으로 실행해 실제 스텝 측정\n",
    "    eval_env = make_env(render_mode=\"rgb_array\", start=START, goal=GOAL)\n",
    "    obs, _ = eval_env.reset()\n",
    "    steps, done = 0, False\n",
    "    while not done and steps < 500:\n",
    "        act, _ = model.predict(obs, deterministic=True)\n",
    "        obs, r, t, tr, _ = eval_env.step(act)\n",
    "        steps += 1\n",
    "        done = t or tr\n",
    "    eval_env.close()\n",
    "    print(f\"Policy steps = {steps}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "test_ship",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
